{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_null(df: DataFrame) -> DataFrame:\n",
    "    \"Filter out entries where the timestamp or text are null\"\n",
    "    return df.where(~(F.isnull(\"created_at\") | F.isnull(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_timestamps(df: DataFrame):\n",
    "    \"Add a `timestamp` and `unix_timestamp` column.\"\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\n",
    "            \"timestamp\",\n",
    "            F.to_timestamp(\"created_at\", \"EEE MMM dd HH:mm:ss Z yyyy\").alias(\"timestamp\"),\n",
    "        )\n",
    "        .withColumn(\"unix_timestamp\", F.unix_timestamp(\"timestamp\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(df: DataFrame, hashtags=True):\n",
    "    \"Split tweet text and make each row represent 1 topic\"\n",
    "    if hashtags:\n",
    "        return df.withColumn(\"topic\", F.explode(F.expr(\"entities.hashtags.text\")))\n",
    "    else:\n",
    "        return (\n",
    "            df\n",
    "            .withColumn(\"split_text\", F.split(\"text\", r\"\\s+\"))\n",
    "            .withColumn(\"topic\", F.explode(\"split_text\"))\n",
    "            .drop(\"split_text\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21321\n",
      "20544\n",
      "3477\n",
      "+-------------------+--------------+---------------+\n",
      "|          timestamp|unix_timestamp|          topic|\n",
      "+-------------------+--------------+---------------+\n",
      "|2011-02-25 15:05:12|    1298642712|    Corinthians|\n",
      "|2011-02-25 15:05:22|    1298642722|         ildivo|\n",
      "|2011-02-25 15:05:42|    1298642742|     centerkita|\n",
      "|2011-02-25 15:06:49|    1298642809|   FollowFriday|\n",
      "|2011-02-25 15:07:52|    1298642872|      viatumblr|\n",
      "|2011-02-25 15:09:04|    1298642944|    ForeverSNSD|\n",
      "|2011-02-25 15:09:04|    1298642944|      ilovesnsd|\n",
      "|2011-02-25 15:11:29|    1298643089|constantcontact|\n",
      "|2011-02-25 15:13:10|    1298643190|           hhrs|\n",
      "|2011-02-25 15:13:28|    1298643208| VampireDiaries|\n",
      "|2011-02-25 15:13:28|    1298643208|            TVD|\n",
      "|2011-02-25 15:14:49|    1298643289|     nowplaying|\n",
      "|2011-02-25 15:15:03|    1298643303|    bacamantera|\n",
      "|2011-02-25 15:15:08|    1298643308|             FF|\n",
      "|2011-02-25 15:15:08|    1298643308|         FFSexy|\n",
      "|2011-02-25 15:15:24|    1298643324|        ACIDMAN|\n",
      "|2011-02-25 15:15:24|    1298643324|        ACIDMAN|\n",
      "|2011-02-25 15:15:35|    1298643335| teamfollowback|\n",
      "|2011-02-25 15:15:56|    1298643356|          CREDO|\n",
      "|2011-02-25 15:15:56|    1298643356|            ARG|\n",
      "+-------------------+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------+-------------------+\n",
      "|     min(timestamp)|     max(timestamp)|\n",
      "+-------------------+-------------------+\n",
      "|2011-02-25 15:05:12|2011-02-27 11:31:24|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"sample.json\")\n",
    "print(df.count())\n",
    "df = filter_null(df)\n",
    "print(df.count())\n",
    "df = parse_timestamps(df)\n",
    "df = get_topics(df, hashtags=True)\n",
    "print(df.count())\n",
    "df = df.select(\"timestamp\", \"unix_timestamp\", \"topic\")\n",
    "\n",
    "df.show()\n",
    "df.select(F.min(\"timestamp\"), F.max(\"timestamp\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_within_timeframe(df: DataFrame, timeframe: str = \"30 minutes\") -> DataFrame:\n",
    "    return (\n",
    "        df\n",
    "        .groupBy(F.window(\"timestamp\", timeframe), \"topic\")\n",
    "        .agg(F.count(\"*\").alias(\"count\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+\n",
      "|              window|       topic|count|\n",
      "+--------------------+------------+-----+\n",
      "|[2011-02-26 17:00...|  cambiochat|   26|\n",
      "|[2011-02-26 13:00...|   FicaDiogo|   12|\n",
      "|[2011-02-26 05:00...|    clericot|   11|\n",
      "|[2011-02-26 07:00...| LongLiveTVD|   10|\n",
      "|[2011-02-25 23:00...|          FF|    8|\n",
      "|[2011-02-25 18:00...|          FF|    8|\n",
      "|[2011-02-25 19:00...|          FF|    8|\n",
      "|[2011-02-25 20:00...|          FF|    7|\n",
      "|[2011-02-25 21:00...|          FF|    7|\n",
      "|[2011-02-26 17:00...|  CambioChat|    6|\n",
      "|[2011-02-25 22:00...|          FF|    6|\n",
      "|[2011-02-25 15:00...|          FF|    6|\n",
      "|[2011-02-26 02:00...|          FF|    6|\n",
      "|[2011-02-25 17:00...|          FF|    6|\n",
      "|[2011-02-26 21:00...|db40birthday|    5|\n",
      "|[2011-02-25 16:00...|          FF|    5|\n",
      "|[2011-02-26 21:00...|  nowplaying|    5|\n",
      "|[2011-02-26 01:00...|          FF|    4|\n",
      "|[2011-02-26 03:00...|          FF|    4|\n",
      "|[2011-02-26 01:00...|          ff|    4|\n",
      "+--------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_within_timeframe(df, \"1 hour\").orderBy(F.desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_slope(df: DataFrame) -> DataFrame:\n",
    "    window = Window.partitionBy(\"topic\").orderBy(F.expr(\"window.start\"))\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\n",
    "            \"slope\",\n",
    "            F.col(\"count\") - F.lag(\"count\").over(window)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+-----+\n",
      "|              window|       topic|count|slope|\n",
      "+--------------------+------------+-----+-----+\n",
      "|[2011-02-26 17:00...|  cambiochat|   11|   10|\n",
      "|[2011-02-25 19:30...|          FF|    6|    4|\n",
      "|[2011-02-25 15:30...|          FF|    5|    4|\n",
      "|[2011-02-25 22:30...|          FF|    5|    4|\n",
      "|[2011-02-26 17:30...|  cambiochat|   15|    4|\n",
      "|[2011-02-26 17:30...|  CambioChat|    5|    4|\n",
      "|[2011-02-25 18:00...|          ff|    4|    3|\n",
      "|[2011-02-25 23:30...|       Libya|    3|    2|\n",
      "|[2011-02-26 01:30...|          FF|    3|    2|\n",
      "|[2011-02-25 17:30...|          FF|    4|    2|\n",
      "|[2011-02-26 23:30...|          np|    3|    2|\n",
      "|[2011-02-26 01:00...|          ff|    3|    2|\n",
      "|[2011-02-26 02:30...|          FF|    4|    2|\n",
      "|[2011-02-26 00:30...|          FF|    3|    2|\n",
      "|[2011-02-26 21:00...|  nowplaying|    3|    2|\n",
      "|[2011-02-25 23:00...|nomesdemotel|    2|    1|\n",
      "|[2011-02-26 04:00...|          FF|    2|    1|\n",
      "|[2011-02-25 21:30...|          FF|    4|    1|\n",
      "|[2011-02-25 16:30...|          FF|    3|    1|\n",
      "|[2011-02-26 22:00...| nowwatching|    2|    1|\n",
      "+--------------------+------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counted = count_within_timeframe(df, \"30 minutes\")\n",
    "with_slope = calculate_slope(counted)\n",
    "with_slope.orderBy(F.desc(\"slope\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def trending_topics(df: DataFrame, timestamp: datetime, n: int = 5) -> DataFrame:\n",
    "    window = Window.orderBy(F.desc(\"slope\"))\n",
    "    return (\n",
    "        df\n",
    "        .where((timestamp >= F.expr(\"window.start\")) & (timestamp < F.expr(\"window.end\")))\n",
    "        .withColumn(\n",
    "            \"trend_rank\", F.rank().over(window)\n",
    "        )\n",
    "        .where(F.col(\"trend_rank\") <= n)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----+-----+----------+\n",
      "|              window|    topic|count|slope|trend_rank|\n",
      "+--------------------+---------+-----+-----+----------+\n",
      "|[2011-02-25 22:30...|       FF|    5|    4|         1|\n",
      "|[2011-02-25 22:30...|  90sswag|    2|    1|         2|\n",
      "|[2011-02-25 22:30...|teamzeeti|    1|    0|         3|\n",
      "|[2011-02-25 22:30...|       ff|    1|    0|         3|\n",
      "|[2011-02-25 22:30...|   random|    1|    0|         3|\n",
      "|[2011-02-25 22:30...|      smh|    1|    0|         3|\n",
      "+--------------------+---------+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trending_topics(with_slope, datetime.fromisoformat(\"2011-02-25 22:34:50\"), 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Output column sentence already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-67fa0c81deff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/repos/trending_topics_maarten_manolis/venv/lib/python3.8/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/repos/trending_topics_maarten_manolis/venv/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/repos/trending_topics_maarten_manolis/venv/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/repos/trending_topics_maarten_manolis/venv/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/repos/trending_topics_maarten_manolis/venv/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Output column sentence already exists."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"sentence\")\n",
    "\n",
    "\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.19.3-cp38-cp38-macosx_10_9_x86_64.whl (15.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.9 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.19.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Users/emanousogiannis/Desktop/repos/trending_topics_maarten_manolis/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|ngrams                                                            |\n",
      "+------------------------------------------------------------------+\n",
      "|[Hi I, I heard, heard about, about Spark]                         |\n",
      "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
      "|[Logistic regression, regression models, models are, are neat]    |\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |filtered            |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "remover.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
